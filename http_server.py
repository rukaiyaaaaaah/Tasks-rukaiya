# -*- coding: utf-8 -*-
"""HTTP-server.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5QU2aEOyz9JAivm-u5npGtcbOJXZm_e
"""

import asyncio

import numpy as np
import uvicorn
from fastapi import Depends, FastAPI, Request
from faster_whisper import WhisperModel
import torch
app = FastAPI()

NUM_WORKERS = 10
MODEL_TYPE = "large-v2"
CPU_THREADS = 4
VAD_FILTER = True

def create_whisper_model() -> WhisperModel:
    whisper = WhisperModel(MODEL_TYPE,
                           device= "cuda",
                           compute_type="float16",
                           num_workers=NUM_WORKERS,
                           download_root="./models")
    print("Loaded model")
    if hasattr(whisper, 'device'):
     print("Model is loaded on:", whisper.device)
    else:
     print("Unable to determine the device. Check the faster_whisper documentation.")

    return whisper

model = create_whisper_model()
print("Loaded model")


async def parse_body(request: Request):
    data: bytes = await request.body()
    return data


def execute_blocking_whisper_prediction(model: WhisperModel, audio_data_array: np.ndarray) -> str:
    segments, _ = model.transcribe(audio_data_array,
                                   beam_size=5,
                                   vad_filter=VAD_FILTER,
                                   vad_parameters=dict(min_silence_duration_ms=1000))
    segments = [s.text for s in segments]
    transcription = " ".join(segments)
    transcription = transcription.strip()
    return transcription

def execute_blocking_whisper_translation(model: WhisperModel, audio_data_array: np.ndarray) -> str:
    segments, _ = model.transcribe(audio_data_array,
                                   beam_size=5,
                                   vad_filter=VAD_FILTER,
                                   task="translate",
                                   vad_parameters=dict(min_silence_duration_ms=1000))
    segments = [s.text for s in segments]
    transcription = " ".join(segments)
    transcription = transcription.strip()
    return transcription

@app.post("/transcribe")
async def predict(audio_data: bytes = Depends(parse_body)):
    # Convert the audio bytes to a NumPy array
    audio_data_array: np.ndarray = np.frombuffer(audio_data, np.int16).astype(np.float32) / 255.0
    try:
        # Run the prediction on the audio data
        result = await asyncio.get_running_loop().run_in_executor(None, execute_blocking_whisper_prediction, model,
                                                                  audio_data_array)

    except Exception as e:
        print(e)
        result = "Error"

    return {"prediction": result}

@app.post("/translate")
async def predict(audio_data: bytes = Depends(parse_body)):
    # Convert the audio bytes to a NumPy array
    audio_data_array: np.ndarray = np.frombuffer(audio_data, np.int16).astype(np.float32) / 255.0

    try:
        # Run the prediction on the audio data
        result = await asyncio.get_running_loop().run_in_executor(None, execute_blocking_whisper_translation, model,
                                                                  audio_data_array)

    except Exception as e:
        print(e)
        result = "Error"

    return {"prediction": result}

if __name__ == "__main__":
    # Run the FastAPI app with multiple threads
    uvicorn.run(app, host="0.0.0.0", port=8008)