# -*- coding: utf-8 -*-
"""Hexa-demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wxUDdbWkmrE_nthIrxs8SYLFizd2umhx
"""

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
from transformers import pipeline

model_id = "openai/whisper-large-v3"  # update with your model id

pipe = pipeline("automatic-speech-recognition", model=model_id)

model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-one-to-many-mmt")
tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-one-to-many-mmt", src_lang="en_XX")

async def transcribe_live_audio(audio_data, state=""):
    # Initialize the OpenAI client

    # Open the audio file in binary read mode
    output = pipe(
        audio_data,
        max_new_tokens=256,
        generate_kwargs={"task": "translate"}
    )
    text = output["text"]
    state += text + " "
    return state, state

async def translate_text(text, target_language):
    target_lang_code = language_codes[target_language]
    model_inputs = tokenizer(text, return_tensors="pt")
    generated_tokens = model.generate(**model_inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_lang_code])
    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]

language_codes = {
    "Telugu" : "te_IN",
    "Hindi": "hi_IN",
    "Bengali": "bn_IN",
    "Tamil": "ta_IN",
    "Marathi": "mr_IN"
}

import gradio as gr
import base64
import time
def image_to_base64(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# Path to your local logo image
logo_path = "ivaani.jpg"  # Change to your local path

# Convert image to base64
logo_base64 = image_to_base64(logo_path)


with gr.Blocks() as demo:
    # Insert custom HTML for the logo at the top-left corner
    gr.HTML(f"""
        <div style="position: relative; background: '#001a33' top: 10px; bottom: '10px'; left: 0px; z-index: 1000;">
            <div style="display: flex; align-items: center;">
              <img src="data:image/png;base64,{logo_base64}" alt="Logo" style="height: 70px; width: 70px;">
              <h1 style="margin-left: 10px;">iVaani</h1>
            </div>

            <p style="margin-left: 2px;">Upload an audio or video file, or record audio to transcribe</p>
        </div>
    """)

    with gr.Row():
        with gr.Column():
            # Live audio recording and transcription
            gr.Interface(
                fn=transcribe_live_audio,
                inputs=[
                    gr.Audio(sources=["microphone"], type="filepath", streaming=True),
                    'state'
                ],
                outputs=[
                    gr.Textbox(label="Real-time Transcription"),
                    "state"
                ],
                live=True
            )

    with gr.Row():
        text_input = gr.Textbox(label="Enter Text for Translation")
        language_dropdown = gr.Dropdown(list(language_codes.keys()), label="Select Target Language")
        translation_output = gr.Textbox(label="Translation")
        translate_button = gr.Button("Translate")
        translate_button.click(translate_text, inputs=[text_input, language_dropdown], outputs=translation_output)

demo.launch(share=True,server_name="0.0.0.0", debug=True)